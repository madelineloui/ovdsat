{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50b3819-440a-4f37-85a7-6117f89df766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os.path as osp\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPModel\n",
    "from torchvision import transforms\n",
    "from argparse import ArgumentParser\n",
    "from utils_dir.backbones_utils import load_backbone, extract_backbone_features, get_backbone_params, load_backbone_and_tokenizer\n",
    "from utils_dir.coco_to_seg import coco_to_seg\n",
    "\n",
    "from build_prototypes import build_object_prototypes\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77afa0d4-f6c6-45d2-8fc7-39e1fd2c0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_coco = '/home/gridsan/manderson/ovdsat/data/dior/train_coco_subset_N5-1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8dae82-2824-441c-8d10-df88c42cf705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'images' section\n",
      "Item exists in images ✅\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open(train_coco, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check if 'images' exists\n",
    "if \"images\" in data:\n",
    "    print(\"Found 'images' section\")\n",
    "\n",
    "    # Check for a specific item, e.g., file_name = \"dog.jpg\"\n",
    "    exists = any(img.get(\"file_name\") == \"21589.jpg\" for img in data[\"images\"])\n",
    "\n",
    "    if exists:\n",
    "        print(\"Item exists in images ✅\")\n",
    "    else:\n",
    "        print(\"Item not found ❌\")\n",
    "else:\n",
    "    print(\"'images' key not found in JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0a93da-fb6a-448b-bd05-495c4bd1005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--store_bg_prototypes'], dest='store_bg_prototypes', nargs=0, const=True, default=False, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='data/simd_subset_10')\n",
    "parser.add_argument('--save_dir', type=str, default='/mnt/ddisk/boux/code/ovdsat/run/classification_benchmark_exp')\n",
    "parser.add_argument('--annotations_file', type=str, default='/mnt/ddisk/boux/code/data/simd/train_coco_subset_N10.json')\n",
    "parser.add_argument('--backbone_type', type=str, default='dinov2')\n",
    "parser.add_argument('--target_size', nargs=2, type=int, metavar=('width', 'height'), default=(602, 602))\n",
    "parser.add_argument('--window_size', type=int, default=224)\n",
    "parser.add_argument('--scale_factor', type=int, default=1)\n",
    "parser.add_argument('--num_b', type=int, default=10, help='Number of background samples to extract per image')\n",
    "parser.add_argument('--k', type=int, default=200, help='Number of background prototypes (clusters for k-means)')\n",
    "parser.add_argument('--store_bg_prototypes', action='store_true', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd606647-780e-4d55-b830-1bbee2811f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "backbone = 'dinov2'\n",
    "dataset = 'dior'\n",
    "N=5\n",
    "M=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d0b17f-3dff-4767-b6e6-dcc613775ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([\n",
    "    '--data_dir', f'{DATA_DIR}/{dataset}/JPEGImages',\n",
    "    '--save_dir', f'run/init_prototypes/boxes/{dataset}_N{N}-{M}',\n",
    "    '--annotations_file', f'{DATA_DIR}/{dataset}/train_coco_subset_N{N}-{M}.json',\n",
    "    '--backbone_type', backbone,\n",
    "    '--target_size', '602', '602',\n",
    "    '--window_size', '224',\n",
    "    '--scale_factor', '1',\n",
    "    '--num_b', '10',\n",
    "    '--k', '200',\n",
    "    '--store_bg_prototypes'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24806c5-aa33-4dd1-a5fd-ed3f80b06af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/init_data/dior_N5-6\n",
      "data/dior/JPEGImages\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Processed category: groundtrackfield\n",
      "Processed category: baseballfield\n",
      "Processed category: bridge\n",
      "Processed category: Expressway-toll-station\n",
      "Processed category: vehicle\n",
      "Processed category: airplane\n",
      "Processed category: airport\n",
      "Processed category: tenniscourt\n",
      "Processed category: trainstation\n",
      "Processed category: storagetank\n",
      "Processed category: stadium\n",
      "Processed category: windmill\n",
      "Processed category: ship\n",
      "Processed category: golffield\n",
      "Processed category: overpass\n",
      "Processed category: chimney\n",
      "Processed category: dam\n",
      "Processed category: basketballcourt\n",
      "Processed category: harbor\n",
      "Processed category: Expressway-Service-area\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Convert COCO annotations to segmentation masks\n",
    "init_data_path = os.path.join('data', 'init_data', args.save_dir.split('/')[-1])\n",
    "coco_to_seg(args.annotations_file, args.data_dir, init_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93b264-85b0-439c-bb7c-4783f427aa36",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec5549b3-7420-47d2-9bf3-e94ae0a9141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data_path = 'data/init_data/test-n'\n",
    "annotations_file = '/home/gridsan/manderson/ovdsat/data/test-n.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a5634b-35b3-47df-b63a-26717736d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/init_data/test-n\n",
      "data/dior/JPEGImages\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Processed category: groundtrackfield\n",
      "Processed category: baseballfield\n",
      "Processed category: stadium\n",
      "Processed category: windmill\n",
      "Processed category: ship\n",
      "Processed category: golffield\n",
      "Processed category: overpass\n",
      "Processed category: chimney\n",
      "Processed category: dam\n",
      "Processed category: basketballcourt\n",
      "Processed category: harbor\n",
      "Processed category: Expressway-Service-area\n",
      "Processed category: bridge\n",
      "Processed category: Expressway-toll-station\n",
      "Processed category: vehicle\n",
      "Processed category: airplane\n",
      "Processed category: airport\n",
      "Processed category: tenniscourt\n",
      "Processed category: trainstation\n",
      "Processed category: storagetank\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "coco_to_seg(annotations_file, f'{DATA_DIR}/{dataset}/JPEGImages', init_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55af35e1-1e24-409e-a099-a80d036d5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/init_data/dior_N5-1\n",
      "data/dior/JPEGImages\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Processed category: groundtrackfield\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert COCO annotations to segmentation masks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m init_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit_data\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39msave_dir\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcoco_to_seg\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotations_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/ovdsat/utils_dir/coco_to_seg.py:61\u001b[0m, in \u001b[0;36mcoco_to_seg\u001b[0;34m(annotation_file, image_directory, save_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m     image_filename_without_ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_filename)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m     mask_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_filename_without_ext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mask\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_filename)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(category_directory, mask_filename), mask)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert COCO annotations to segmentation masks\n",
    "init_data_path = os.path.join('data', 'init_data', args.save_dir.split('/')[-1])\n",
    "coco_to_seg(args.annotations_file, args.data_dir, init_data_path)\n",
    "\n",
    "\n",
    "# Load model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = load_backbone(args.backbone_type)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "patch_size, _ = get_backbone_params(args.backbone_type)\n",
    "\n",
    "# Build object prototypes\n",
    "obj_category_dict = build_object_prototypes(args, model, init_data_path, device, patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec418313-e3e5-4a3b-8a1e-e970efd06566",
   "metadata": {},
   "source": [
    "### Text (zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bd764f1-4aff-4d90-ae31-fceaa4fc5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prot = torch.load('/home/gridsan/manderson/ovdsat/run/text_prototypes/boxes/dior/prototypes_remoteclip-14.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c664fd5-6fd7-49ed-8019-205b252d058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "print(zero_shot_prot['prototypes'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5766c-e9ad-4cef-b10b-af57e321cd51",
   "metadata": {},
   "source": [
    "### CoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5d454d7-ab0d-484f-b2c2-2909bdf999c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_path = '/home/gridsan/manderson/ovdsat/CoOp/output/dior/CoOp/vit_l14_remote-ep100-ctx2_5shots/nctx4_cscFalse_ctpmiddle/seed1/prompt_learner/model.pth.tar-100'\n",
    "model, tokenizer = load_backbone_and_tokenizer('remoteclip-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b4fcab-85a2-4766-8e14-d663a580cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.load(context_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1693ca00-c651-45df-a3ab-e47cca725a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state_dict', 'epoch', 'optimizer', 'scheduler', 'val_result'])\n"
     ]
    }
   ],
   "source": [
    "print(context.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2ffbdd-d76c-4948-98ad-20a6212fc6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['ctx', 'token_prefix', 'token_suffix'])\n"
     ]
    }
   ],
   "source": [
    "print(context['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdaddc30-f467-41d2-b3d9-debaa6ce759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = context['state_dict']['token_prefix']\n",
    "ctx = context['state_dict']['ctx']\n",
    "suffix = context['state_dict']['token_suffix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73bb09e-2423-4619-9253-550b34820436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 768])\n",
      "torch.Size([4, 768])\n",
      "torch.Size([20, 72, 768])\n"
     ]
    }
   ],
   "source": [
    "print(prefix.shape)\n",
    "print(ctx.shape)\n",
    "print(suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3cd4c3-964f-4c1e-905e-5a3e37689a7b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for end, class specific\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (n_cls, 1, dim)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# (n_cls, n_ctx, dim)\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (n_cls, *, dim)\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m prompts\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "# # for end, class specific\n",
    "# prompts = torch.cat(\n",
    "#     [\n",
    "#         prefix,  # (n_cls, 1, dim)\n",
    "#         ctx,     # (n_cls, n_ctx, dim)\n",
    "#         suffix,  # (n_cls, *, dim)\n",
    "#     ],\n",
    "#     dim=1,\n",
    "# )\n",
    "# prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741f0992-a961-49e9-83a1-b3d77ff86a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "classes = ['expressway service area', 'expressway toll station', 'airplane', 'airport', 'background', 'baseball field', 'basketball court', 'bridge', 'chimney', 'dam', 'golf field', 'ground track field', 'harbor', 'overpass', 'ship', 'stadium', 'storage tank', 'tennis court', 'train station', 'vehicle', 'windmill']\n",
    "for name in classes:\n",
    "    tokens = len(tokenizer.encode(name))\n",
    "    print(tokens)\n",
    "    # token_embed = model.token_embedding(tokens)\n",
    "    # print(token_embed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e77bf0-d969-4559-9a84-1952866eb4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 3, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 2, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 77, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for middle, unified\n",
    "classes = ['ground track field', 'baseball field', 'bridge', 'expressway toll station', 'vehicle', 'airplane', 'airport', 'tennis court', 'train station', 'storage tank', 'stadium', 'windmill', 'ship', 'golf field', 'overpass', 'chimney', 'dam', 'basketball court', 'harbor', 'expressway service area']\n",
    "name_lens = [len(tokenizer.encode(name)) for name in classes]\n",
    "n_ctx = 4\n",
    "n_cls = 20\n",
    "\n",
    "if ctx.dim() == 2:\n",
    "    ctx = ctx.unsqueeze(0).expand(n_cls, -1, -1)\n",
    "\n",
    "half_n_ctx = n_ctx // 2\n",
    "prompts = []\n",
    "for i in range(n_cls):\n",
    "    name_len = name_lens[i]\n",
    "    prefix_i = prefix[i : i + 1, :, :]\n",
    "    class_i = suffix[i : i + 1, :name_len, :]\n",
    "    print(class_i.shape)\n",
    "    suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "    ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "    ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "    prompt = torch.cat(\n",
    "        [\n",
    "            prefix_i,     # (1, 1, dim)\n",
    "            ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "            class_i,      # (1, name_len, dim)\n",
    "            ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "            suffix_i,     # (1, *, dim)\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "prompts = torch.cat(prompts, dim=0)\n",
    "prompts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500dc21d-54b2-4988-9511-15cb2993f79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 77, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder = model.transformer\n",
    "text_feats = text_encoder(prompts.to('cpu'))\n",
    "text_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e72fd8a-9a27-4a8e-a187-6e04e533be16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "text_feats = text_feats[:, 0, :] #CLS token\n",
    "text_feats.shape # Final shape of text prototypes should be [n_classes, dim=768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0474fb0-10c8-43b3-ba1d-eaf7ea1d9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_classes = ['Ground Track Field', 'Baseball Field', 'Bridge', 'Expressway Toll Station', 'Vehicle', 'Airplane', 'Airport', 'Tennis Court', 'Train Station', 'Storage Tank', 'Stadium', 'Windmill', 'Ship', 'Golf Field', 'Overpass', 'Chimney', 'Dam', 'Basketball Court', 'Harbor', 'Expressway Service Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bfb3fd-5161-4be3-a654-6c0ab61cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/gridsan/manderson/ovdsat/data/text/dior_labels.txt', \"r\") as f:\n",
    "        classes = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26060d-5844-40f5-b1a2-64a0f0b02918",
   "metadata": {},
   "source": [
    "### Understanding CoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5e8e50-3db0-420a-ae18-2ba4a2d59383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7bd623e-b26e-4329-ad42-a7a0e156bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctx = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55d083fe-f308-4a3b-a056-e3f345875a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classnames = ['airplane', 'boat', 'car']\n",
    "name_lens = [len(open_clip.tokenize(name)) for name in classnames]\n",
    "name_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df850636-e081-438e-b45d-1bf0f8801d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X X X X X X X X X X X X X X X X'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "prompt_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "916c50c0-4f04-450f-922a-cd3d4048ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['X X X X X X X X X X X X X X X X airplane.',\n",
       " 'X X X X X X X X X X X X X X X X boat.',\n",
       " 'X X X X X X X X X X X X X X X X car.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "print(len(prompts[0]))\n",
    "prompts # Prompts [n_classes, num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87f50314-8aee-4f5e-af19-f5868bb5344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_prompts = torch.cat([open_clip.tokenize(p) for p in prompts])\n",
    "tokenized_prompts.shape # Tokenized prompts [n_classes, num_tokens] (num_tokens usually 77 with padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57ba7f33-8a43-4947-9ab4-d560a17b8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49406,   343,   343,   343,   343,   343,   343,   343,   343,   343,\n",
      "          343,   343,   343,   343,   343,   343,   343, 16451,   269, 49407,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff365bd9-6c81-4e9b-8cbb-2c151aef44be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = model.token_embedding(tokenized_prompts)\n",
    "embedding.shape # Token embeddings [n_classes, num_tokens, dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e5cc9-5c05-4569-a887-58ebe856c5ed",
   "metadata": {},
   "source": [
    "### Create Eurosat json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a759f6d6-d0a9-478b-a39f-8c325d598b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(\"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/train.csv\")\n",
    "train_df.reset_index(drop=True, inplace=True) \n",
    "val_df = pd.read_csv(\"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/validation.csv\")\n",
    "val_df.reset_index(drop=True, inplace=True) \n",
    "test_df = pd.read_csv(\"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/test.csv\")\n",
    "test_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "# Remove columns that are unnamed or have 'Unnamed' in their names\n",
    "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
    "val_df = val_df.loc[:, ~val_df.columns.str.contains('^Unnamed')]\n",
    "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Ensure the column names are consistent with the expected format (impath, label, classname)\n",
    "train_df.columns = ['impath', 'label', 'classname']\n",
    "val_df.columns = ['impath', 'label', 'classname']\n",
    "test_df.columns = ['impath', 'label', 'classname']\n",
    "\n",
    "# Convert DataFrames to list of lists\n",
    "train_data = train_df.values.tolist()\n",
    "val_data = val_df.values.tolist()\n",
    "test_data = test_df.values.tolist()\n",
    "\n",
    "# Combine into one dictionary\n",
    "dataset = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data,\n",
    "    \"test\": test_data\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/dataset.json\", \"w\") as json_file:\n",
    "    json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3e982f-3482-4535-bdc8-abf405af09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dassl.utils import read_json\n",
    "\n",
    "split = read_json(\"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac93920-0cfd-4c06-9c0e-e070715cda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = split['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33904c84-43ec-4dd1-ae12-c5dd52700173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AnnualCrop/AnnualCrop_142.jpg', 0, 'AnnualCrop']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260c6d8f-8fec-43a2-bb58-f44a1ab7b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnualCrop/AnnualCrop_142.jpg 0 AnnualCrop\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for impath, label, classname in train_split:\n",
    "    if i < 1:\n",
    "        print(impath, label, classname)\n",
    "    else:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa17564-5d63-4ec5-86e7-d94ce35ffd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9debed3-453a-41ee-b4b7-e5f389429ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/manderson/.conda/envs/dassl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gridsan/manderson/.conda/envs/dassl/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/gridsan/manderson/.conda/envs/dassl/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e58e53-831a-47c8-b824-505a0c7f49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained('/home/gridsan/manderson/ovdsat/weights/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed54efb-7799-4d32-9122-71a48088c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad887faa-b9c0-4f10-87b0-e7ff6988cbc0",
   "metadata": {},
   "source": [
    "### Convert dassl datums to csv splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3017e7-d6f7-446c-bd18-0532eb3a01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034e81f1-ac2c-4d58-8a59-9e6a8f2d3db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [<dassl.data.datasets.base_dataset.Datum at 0x7fa0cc85ba60>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc86e130>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc86e250>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc868940>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc879730>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc8796d0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc8794f0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc879430>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc879460>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc879910>],\n",
       " 'val': [<dassl.data.datasets.base_dataset.Datum at 0x7fa0cc74f430>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f863bf6a0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7fa0cc75dbb0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f860aad00>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e9011c0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e901160>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e901970>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e901a60>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e901be0>,\n",
       "  <dassl.data.datasets.base_dataset.Datum at 0x7f9f7e9139d0>]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/gridsan/manderson/ovdsat/data/eurosat/split_fewshot/shot_1-seed_1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data # has train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e4385b-f07e-4cfc-b70b-5960a2efabc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/AnnualCrop/AnnualCrop_832.jpg\n",
      "0\n",
      "AnnualCrop\n"
     ]
    }
   ],
   "source": [
    "print(data['train'][0]._impath)\n",
    "print(data['train'][0]._label)\n",
    "print(data['train'][0]._classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27a9763b-fd5b-4f9f-b820-17d388a07ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a .pkl file and save train/val sets as CSV\n",
    "def convert_pkl_to_csv(pkl_path):\n",
    "    # Extract shot and seed from filename (assumes format: shot_X-seed_Y.pkl)\n",
    "    filename = os.path.basename(pkl_path).replace(\".pkl\", \"\")\n",
    "    print(filename)\n",
    "    \n",
    "    # Load the .pkl file\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Extract train and validation sets\n",
    "    train_set = data.get(\"train\", [])  # Default to empty list if missing\n",
    "    val_set = data.get(\"val\", [])  # Default to empty list if missing\n",
    "    \n",
    "    # Function to extract relevant attributes from datum objects\n",
    "    def extract_data(dataset):\n",
    "        return [{\"Filename\": d._impath, \"Label\": d._label, \"ClassName\": d._classname} for d in dataset]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    train_df = pd.DataFrame(extract_data(train_set))\n",
    "    val_df = pd.DataFrame(extract_data(val_set))\n",
    "\n",
    "    # Save to CSV\n",
    "    train_csv_path = f\"{pkl_path[:-4]}-train.csv\"\n",
    "    val_csv_path = f\"{pkl_path[:-4]}-val.csv\"\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved: {train_csv_path}, {val_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e892034-3308-487c-8ad5-1f5a81b8e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot_2-seed_5\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_5-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_5-val.csv\n",
      "shot_1-seed_3\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_3-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_3-val.csv\n",
      "shot_4-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_1-val.csv\n",
      "shot_4-seed_5\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_5-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_5-val.csv\n",
      "shot_2-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_1-val.csv\n",
      "shot_1-seed_4\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_4-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_4-val.csv\n",
      "shot_16-seed_2\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_2-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_2-val.csv\n",
      "shot_16-seed_3\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_3-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_3-val.csv\n",
      "shot_16-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_1-val.csv\n",
      "shot_5-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_5-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_5-seed_1-val.csv\n",
      "shot_16-seed_4\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_4-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_4-val.csv\n",
      "shot_4-seed_2\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_2-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_2-val.csv\n",
      "shot_8-seed_5\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_5-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_5-val.csv\n",
      "shot_8-seed_3\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_3-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_3-val.csv\n",
      "shot_4-seed_3\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_3-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_3-val.csv\n",
      "shot_2-seed_2\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_2-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_2-val.csv\n",
      "shot_1-seed_2\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_2-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_2-val.csv\n",
      "shot_2-seed_4\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_4-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_4-val.csv\n",
      "shot_2-seed_3\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_3-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_2-seed_3-val.csv\n",
      "shot_1-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_1-val.csv\n",
      "shot_8-seed_4\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_4-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_4-val.csv\n",
      "shot_1-seed_5\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_5-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_1-seed_5-val.csv\n",
      "shot_16-seed_5\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_5-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_16-seed_5-val.csv\n",
      "shot_8-seed_1\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_1-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_1-val.csv\n",
      "shot_4-seed_4\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_4-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_4-seed_4-val.csv\n",
      "shot_8-seed_2\n",
      "Saved: /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_2-train.csv, /home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot/shot_8-seed_2-val.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "dir_path = \"/home/gridsan/manderson/ovdsat/data/eurosat/EuroSAT/split_fewshot\"\n",
    "\n",
    "# Find all .pkl files in the directory\n",
    "pkl_files = glob.glob(f\"{dir_path}/*.pkl\")\n",
    "\n",
    "for pkl_file in pkl_files:\n",
    "    convert_pkl_to_csv(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deae35-8ca6-42b6-a0e6-fc9d53fa51b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovdsat",
   "language": "python",
   "name": "ovdsat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
